{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f951f46-b561-44d0-b0e8-e94e59f74666",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation with SageMaker\n",
    "\n",
    "Text to text Generative AI models have a well documented problem which is the issue of having only information up to the date for which they were trained. This notebook shows how to use retrieval augmented generation (RAG), otherwise known as data augmented generation, to help suppliment text generation models with up to date information via document search. We will use two different models to do this. First, we will use the HuggingFace FLAN T5 for document and question embedding. Second, we will use AI21 Lab's Jurassic Instructor Jumbo model for text generation. \n",
    "\n",
    "**Please note: this notebook requires access to the foundation models in SageMaker Jumpstart which is in private preview at the time of writing.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8319576-7def-4496-af23-b59621bebb6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setup Environment\n",
    "\n",
    "We will install a few libraries and import necessary packages for the notebook. We will use the `transformers` library to produce our embeddings and the `ai21` lab to interact with the Jurassic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a84623-fe16-4fc1-ac81-0713011c915b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install setuptools~=46.0.0 --quiet\n",
    "!pip install \"ai21[SM]\" --quiet\n",
    "!pip install torch transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781c66de-7fbd-4481-b370-68704186248b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, T5EncoderModel\n",
    "import torch\n",
    "import ai21\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sagemaker import ModelPackage\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import ModelPackage\n",
    "import sagemaker\n",
    "\n",
    "import json\n",
    "import boto3\n",
    "import requests\n",
    "import logging\n",
    "\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "logger = logging.getLogger('sagemaker')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "url = \"https://nm3yyjazj1.execute-api.us-east-1.amazonaws.com/Prod/invoke\"\n",
    "\n",
    "role = get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "runtime_sm_client = boto3.client(\"runtime.sagemaker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8846c3a3-04b0-4885-aa92-8e1297f760ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_endpoint_with_json_payload(url, payload):\n",
    "    response = requests.post(\n",
    "        url,\n",
    "        json=payload,\n",
    "    )\n",
    "    #print(payload)\n",
    "    return response\n",
    "\n",
    "def parse_response_multiple_texts(query_response):\n",
    "    model_predictions = query_response.json()\n",
    "    #print(query_response)\n",
    "    generated_text = model_predictions['message']\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb731ad-ebdf-4aa6-b137-3e6aa9e0d614",
   "metadata": {},
   "source": [
    "# Deploy Jurassic Model to SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0404e0fa-4129-4248-a113-264cbda9b1fe",
   "metadata": {},
   "source": [
    "## Optional - Retrieve & deploy the jurrasic image for deployment \n",
    "The first step is to set up a SageMaker session and collect the Jurassic Jumbo Instruct model ARN. Use the cells below to deploy the model. In this particular use case we have already deployed and exposed a jurrasic model via an API GW so we can skip the cells below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937ef7b9-b98d-4455-98e4-eadb00318862",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_package_map = {\n",
    "    \"us-east-1\": \"arn:aws:sagemaker:us-east-1:865070037744:model-package/j2-jumbo-instruct-v1-0-20-8b2be365d1883a15b7d78da7217cdeab\",\n",
    "}\n",
    "region = boto3.Session().region_name\n",
    "model_package_arn = model_package_map[region]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7362e16-ece7-4730-98c0-7f69d393d4d0",
   "metadata": {},
   "source": [
    "## Deploy Endpoint\n",
    "\n",
    "You can now deploy the Jurassic model to a SageMaker endpoint in order to send text into the model in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273a2bed-4c0b-432c-ac0d-b21bb89f2b78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"j2-jumbo-instruct\"\n",
    "content_type = \"application/json\"\n",
    "real_time_inference_instance_type = (\n",
    "    \"ml.g5.48xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b50c08a-2d13-4a90-8594-fa445fa1d5e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a deployable model from the model package.\n",
    "model = ModelPackage(\n",
    "    role=role, model_package_arn=model_package_arn, sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592b3e30-1945-4b84-a7dc-f1341f4e80ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# uncomment the bwlow lines to deploy the model. In this use case we have already deployed a jurrasic mid model and exposed via API GW.\n",
    "# Deploy the model\n",
    "#predictor = model.deploy(\n",
    "#    1, real_time_inference_instance_type, endpoint_name=model_name, \n",
    "#    model_data_download_timeout=3600,\n",
    "#    container_startup_health_check_timeout=600,\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8a2d2c-dfdb-4c64-bfcb-06ac368cc5d8",
   "metadata": {},
   "source": [
    "# The Hallucination Issue\n",
    "\n",
    "Now that we have an endpoint up and running, the example below shows how the Jurassic model \"hallucinates\" that France won the 2022 world cup. The actual fact is that Argentina won in 2022 and France won in 2018. Here in lies the problem we need to fix with RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0db0b0c-9d66-48ae-acd8-8a32a7959b03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "prompt = f'''Answer the following question.\n",
    "Question: Who won the 2022 world cup?\n",
    "Answer:\n",
    "'''\n",
    "\n",
    "payload = {\"prompt\":prompt, \"max_token\":200, \"temperature\": 0}\n",
    "response = query_endpoint_with_json_payload(url, payload)\n",
    "\n",
    "generated_text = parse_response_multiple_texts(response)\n",
    "logger.info(f'Generated text: \\n{generated_text}')\n",
    "\n",
    "#response = ai21.Completion.execute(\n",
    "#    sm_endpoint=\"j2-jumbo-instruct\",\n",
    "#    prompt=prompt,\n",
    "#    maxTokens=200,\n",
    "#    temperature=0,\n",
    "#    numResults=1\n",
    "#)\n",
    "#print(response['completions'][0]['data']['text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fa7a33-0829-49fd-9c23-7900620bf6f3",
   "metadata": {},
   "source": [
    "In this cell promt engineering with adding the line `If you do not have the information to answer the question, say \"I don't know\".` to the prompt produces the answer of \"I don't know\" which is better than producing a wrong answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5edd310-9bf6-4451-81f4-c35e4aaced6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f'''Answer the following question. If you do not have the information to answer the question, say \"I don't know\".\n",
    "Question: Who won the 2022 world cup?\n",
    "Answer:\n",
    "'''\n",
    "\n",
    "payload = {\"prompt\":prompt, \"max_token\":200, \"temperature\": 0}\n",
    "response = query_endpoint_with_json_payload(url, payload)\n",
    "\n",
    "generated_text = parse_response_multiple_texts(response)\n",
    "logger.info(f'Generated text: \\n{generated_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b58c05c-dcd9-41fb-bbd8-ea05ee77cb12",
   "metadata": {},
   "source": [
    "# Get a HuggingFace Model for Embeddings\n",
    "\n",
    "Load in the [FLAN T5 large model](https://huggingface.co/google/flan-t5-large) from HuggingFace. This will be the model we use to create our document search embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76da004-f9b4-4836-bab3-11034bee17ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this will download the flan t5 tokenizer for use of embedding. This may take a few moments to download. \n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = T5EncoderModel.from_pretrained(\"google/flan-t5-large\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2e9d49-5d5a-4d85-832c-d40b9bb8dd90",
   "metadata": {},
   "source": [
    "# Create Embedding Database\n",
    "\n",
    "Now use the HuggingFace model to create embeddings for each of the three documents which have been provided. The documents used here are only illustrative. These documents could be extended to any collection of text to help supplement your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4f13d4-7f74-4302-8f4f-dc694e3c81e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_embedding(text, model, tokenizer):\n",
    "    with torch.no_grad():\n",
    "        input_ids = tokenizer(\n",
    "            text, return_tensors=\"pt\", max_length=512, truncation=True\n",
    "        ).input_ids.to(DEVICE)\n",
    "        outputs = model(input_ids=input_ids)\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        e = last_hidden_states.mean(dim=1)\n",
    "    return e\n",
    "\n",
    "# helper function to create the document embeddings from a document\n",
    "def create_doc_database(docs, model, tokenizer):\n",
    "    database = []\n",
    "    for i in range(docs.shape[0]):\n",
    "        text = docs['title'].values[i] + ' - ' + docs['document'].values[i]\n",
    "        e = get_embedding(text, model, tokenizer)\n",
    "        database.append(e)\n",
    "    database = torch.cat(database)\n",
    "    return database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1494a60-f483-48c7-9b35-26d9d58cf2a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = pd.read_csv('document-corpus.txt', delimiter=\"::: \", engine='python')\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66af58ab-23c6-4a5d-aaa2-4a9feb908150",
   "metadata": {},
   "source": [
    "### Embeddings \n",
    "\n",
    "So far we have taken the document corpus which is csv file containing two rows with results from world cup 2022, champions league and ballon dor. We are simply using the document text and passing it to the tokenizer to get the embeddings. \n",
    "\n",
    "The end result is a vector representation of each document which has 1024 floating points. We will use this vector embedding to load into a database for search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a915a9c-9cc5-4bdb-92c4-1416fbc396ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "database = create_doc_database(docs, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6935bbc8-18d3-4ff1-8fd8-35226e02a055",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "database.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c316a51-1df2-4a25-805c-f5a323de271b",
   "metadata": {},
   "source": [
    "# Create Search Ability\n",
    "\n",
    "Now that you have a database of embeddings, we can search the database against a text input `\"Who won the 2022 world cup?\"` to see which document is most relevant to the question by looking at the dot product of the embeddings. Behind the scene, we have to convert to query/search request to a vector embedding using the same tokenizer used to create the document embedding and search for similarities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1ef481-c6e2-4a6a-b2b1-644b1782c163",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def search_database(search_embedding, database):\n",
    "    similarities = []\n",
    "    for i in range(database.shape[0]):\n",
    "        similarities.append(\n",
    "            float(torch.dot(search_embedding[0], database[i]))\n",
    "        )\n",
    "    return np.argmax(similarities), similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e075b2-591e-4348-b135-1708e6600539",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "search = 'Who won the 2022 world cup?'\n",
    "search_embedding = get_embedding(search, model, tokenizer)\n",
    "doc_index, similarities = search_database(search_embedding, database)\n",
    "print(f\"Input: {search}\\nWas matched with document #{doc_index} which is titled \\\"{docs.loc[doc_index]['title']}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d6bbf7-adf0-436c-aa76-c900d7d1a8c3",
   "metadata": {},
   "source": [
    "# Dynamically Engineer the Prompt\n",
    "\n",
    "Now that we have a user input matched with a relevant document, we can engineer a prompt which includes both the question and context from the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78f1d20-2a3c-4a29-a68c-7cfd1b6f0657",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_eng_base = '''Answer the following question with the following context. If you do not have the information to answer the question, say \"I don't know\".\n",
    "\n",
    "Context: [PLACE DOC HERE]\n",
    "\n",
    "Question: [PLACE QUESTION HERE]\n",
    "Answer:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd1702d-abec-43ce-a555-d39150c6818e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_prompt(search, context, prompt_eng_base):\n",
    "    prompt = prompt_eng_base.replace('[PLACE DOC HERE]', context)\n",
    "    prompt = prompt.replace('[PLACE QUESTION HERE]', search)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde37f7b-4284-4045-91d4-f17efbd16dda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_custom = make_prompt(search, docs.loc[doc_index]['document'], prompt_eng_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b0bdf8-540b-464f-a556-97f26a147d0c",
   "metadata": {},
   "source": [
    "# Wrap the RAG Flow into a Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edb71c1-af16-4e14-b305-ea3f327c3d9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_prompt = f'''Answer the following question. If you do not have the information to answer the question, say \"I don't know\".\n",
    "\n",
    "Question: [SEARCH HERE]\n",
    "Answer:\n",
    "'''\n",
    "\n",
    "def rag_demo(search, use_search=True):\n",
    "    search_embedding = get_embedding(search, model, tokenizer)\n",
    "    doc_index, similarities = search_database(search_embedding, database)\n",
    "    if use_search:\n",
    "        prompt_custom = make_prompt(search, docs.loc[doc_index]['document'], prompt_eng_base)\n",
    "    else:\n",
    "        prompt_custom = base_prompt.replace('[SEARCH HERE]', search)\n",
    "        \n",
    "    payload = {\"prompt\":prompt_custom, \"max_token\":200, \"temperature\": 0}\n",
    "    response = query_endpoint_with_json_payload(url, payload)\n",
    "\n",
    "    generated_text = parse_response_multiple_texts(response)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f239b9-1eab-4cc3-a79a-4af44a6f1082",
   "metadata": {},
   "source": [
    "# Example Outputs\n",
    "\n",
    "The outputs below show how you can now get relevant information to the model in order to give informed responses back to the user!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d984d679-a8bf-42af-92b5-a31e95eaa573",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = rag_demo('Who won the 2022 world cup?', use_search=False)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c285e66-eb66-423e-9536-db5482268594",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = rag_demo('Who won the 2022 world cup?', use_search=True)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfa3c01-8dda-4241-8a99-94210220070b",
   "metadata": {},
   "source": [
    "# Suggested Next Steps\n",
    "\n",
    "* Explore libraries which can help with this kind of workflow. See: [LangChain](https://github.com/hwchase17/langchain)\n",
    "* Bring your own documents or information to this workflow to explore creating RAG based systems.\n",
    "* Look into fine tuning your embedding model to produce better searching.\n",
    "* Integrate this RAG flow with integrations to your own search capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6bc231-48d9-4b84-bc52-7f9c8ec2a602",
   "metadata": {},
   "source": [
    "# Cleanup \n",
    "\n",
    "Uncomment the cells below to delete the endpoint if an endpoint was deployed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1f64e8-73ac-4baf-bed4-3c9abac92369",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#j2 = sagemaker.predictor.Predictor('j2-jumbo-instruct\"')\n",
    "#j2.delete_model()\n",
    "#j2.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e72fe76-2671-418f-bee1-4418c25b742e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
